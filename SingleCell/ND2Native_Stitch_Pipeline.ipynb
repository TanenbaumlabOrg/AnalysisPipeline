{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ND2 native stitch pipeline\n",
    "This notebook handles a single nd2 file, it must be a multiposition & timelapse dataset.\n",
    "\n",
    "## Key details:\n",
    "- Sequential positions are individual tile images belonging to discrete, rectangular & non-overlapping tile regions (panoramas). These are to be identified, stitched, and processed in this notebook.\n",
    "- No Z-interpretation, maxIP is executed before (NIS) or during pipeline execution.\n",
    "- During feature extraction (spot detection & intensity extraction) tile images are considered independently. Features are later taken only for the best representation of each cell in tile image overlap regions.\n",
    "\n",
    "## Key pipeline components:\n",
    "- ND2 parsing by [nd2](https://github.com/tlambert03/nd2)\n",
    "  - Dataset layout\n",
    "  - Metadata\n",
    "- (optional) flatfield illumination correction\n",
    "- Tile region identification using stage coordinates and pixel size (metadata)\n",
    "- Tile region stitching by either:\n",
    "  - Hard overlap according to stage coordinates, no registration\n",
    "  - Image registration with [MIST](https://github.com/usnistgov/MIST)\n",
    "- Segmentation of nuclei (& cells) by [CellPose](https://github.com/MouseLand/cellpose)\n",
    "- Cell tracking by either:\n",
    "  - [TrackMate](https://github.com/trackmate-sc/TrackMate)\n",
    "  - [TrackAstra](https://github.com/weigertlab/trackastra/)\n",
    "- Spot detection by either:\n",
    "  - [SpotiFlow](https://github.com/weigertlab/spotiflow)\n",
    "  - [DeepBlink](https://github.com/bbquercus/deepblink/) **! depricated here, use alternative env & notebook**\n",
    "- Cell representation in tile image overlap\n",
    "- Cell intensity and morphology feature extraction\n",
    "- Data visualization with Napari\n",
    "- Optional video rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.contrib.itertools import product\n",
    "\n",
    "import nd2\n",
    "from tifffile import imread, imwrite\n",
    "import imagej\n",
    "from skimage import measure\n",
    "\n",
    "import napari\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# find path to function imports\n",
    "from pathlib import Path\n",
    "path_imports = str(Path(os.getcwd()).resolve().parents[0]) + '/src/'\n",
    "sys.path.append(path_imports)\n",
    "\n",
    "# import external function\n",
    "import importlib\n",
    "import MetadataParserND2\n",
    "import TileRegion\n",
    "import FlatDarkField\n",
    "import LoopDimensions\n",
    "import SegmentationCellpose\n",
    "import LinkPointToObject\n",
    "import FeatureExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of FIJI\n",
    "path_imagej_mist = '/data/Fiji.app'\n",
    "if not os.path.exists(path_imagej_mist):\n",
    "    print('Path invalid!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trackmate imagej macro path\n",
    "path_script_Trackmate = path_imports + \"trackmate_init.py\"\n",
    "if not os.path.exists(path_script_Trackmate):\n",
    "    print('Path invalid!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data path definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset to process\n",
    "path_nd2 = '/dummy.nd2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder with flatfield correction data\n",
    "path_ff_base = '/dummy/flatfield/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis folder setup\n",
    "name = os.path.basename(path_nd2).split('.')[0]\n",
    "path_base = os.path.dirname(path_nd2)\n",
    "\n",
    "path_export_tile = path_base + \"/Export/\"\n",
    "path_export_stitch = path_base + \"/Stitch/\"\n",
    "path_analysis_segmentation = path_base + \"/Masks/\"\n",
    "path_analysis_tracks = path_base + \"/Tracks/\"\n",
    "path_analysis_spots = path_base + \"/Spots/\"\n",
    "path_features = path_base + \"/Features/\"\n",
    "\n",
    "os.makedirs(path_export_tile, exist_ok=True)\n",
    "os.makedirs(path_export_stitch, exist_ok=True)\n",
    "os.makedirs(path_analysis_segmentation, exist_ok=True)\n",
    "os.makedirs(path_analysis_tracks, exist_ok=True)\n",
    "os.makedirs(path_analysis_spots, exist_ok=True)\n",
    "os.makedirs(path_features, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load nd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User toggles\n",
    "flip_x = False\n",
    "flatfield = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse metadata\n",
    "sizes, channels, positions, px_size = MetadataParserND2.get_nd2_meta(path_nd2)\n",
    "\n",
    "# first loop of positional redundant dimensions (T/Z) lists unique stage positions\n",
    "positions_first = positions[(positions['iT'] == 0) & (positions['iZ'] == 0)].copy()\n",
    "data_img = nd2.ND2File(path_nd2).to_dask()\n",
    "\n",
    "sizes = dict(sizes)\n",
    "print(sizes)\n",
    "print(channels)\n",
    "print(data_img.shape)\n",
    "positions_first.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) bind image position names (incl well codes) to positions_first df\n",
    "positions_first['name'] = MetadataParserND2.get_nd2_tile_names(path_nd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse metadata, show camera settings\n",
    "print(MetadataParserND2.get_nd2_camera_settings(path_nd2))\n",
    "print(MetadataParserND2.get_nd2_misc(path_nd2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeframe to use for stitching, default: half of timelapse length\n",
    "t_stitch = int(round(sizes.get('T', 1)/2))\n",
    "print(t_stitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For single channel data place a dummy channel axis\n",
    "if 'C' not in sizes:\n",
    "    data_img, sizes = MetadataParserND2.fix_nd2_single_channel(data_img, sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Z dimension by maxIP\n",
    "if sizes.get('Z', 1) > 1:\n",
    "    data_img, sizes = MetadataParserND2.proj_nd2_max(data_img, sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stitch\n",
    "channelsOI_stitch = 0\n",
    "print(\"Stitch: \" + str(np.array(channels)[channelsOI_stitch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation (nuclei, cyto optional)\n",
    "channelsOI_cellpose_nucl = 0\n",
    "channelsOI_cellpose_cell = None #None to disable\n",
    "\n",
    "print(\"Cellpose NUCL: \", str(np.array(channels)[channelsOI_cellpose_nucl]))\n",
    "if channelsOI_cellpose_cell != None:\n",
    "    print(\"Cellpose CELL: \", str(np.array(channels)[channelsOI_cellpose_cell]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot desection\n",
    "channelsOI_spots = [True, False]\n",
    "print(\"Deepblink: \" + str(np.array(channels)[channelsOI_spots]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flat field loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable flatfield correction\n",
    "if not flatfield:\n",
    "    img_ff = []\n",
    "    img_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camera metadata (sensor crop)\n",
    "with nd2.ND2File(path_nd2) as nd2file:\n",
    "    nd2meta = nd2file.unstructured_metadata()['ImageMetadataSeqLV|0']['SLxPictureMetadata']['PicturePlanes']['SampleSetting']\n",
    "    nd2file.close()\n",
    "max_key = max(int(key) for key in nd2meta.keys())\n",
    "crop_cam = nd2meta[str(max_key)]['CameraSetting']['ROI']\n",
    "print(crop_cam)\n",
    "print(MetadataParserND2.get_nd2_camera_settings(path_nd2))\n",
    "print(channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process full-frame flatfield images with the recorded sensor crop\n",
    "ff_custom = True\n",
    "\n",
    "if ff_custom:\n",
    "    crop_y = crop_cam['Top']\n",
    "    crop_x = crop_cam['Left']\n",
    "    crop_w = crop_cam['Right'] - crop_cam['Left']\n",
    "    crop_h = crop_cam['Bottom'] - crop_cam['Top']\n",
    "    \n",
    "    img_ff_BFP = imread(path_ff_base + 'BFP.tiff')[crop_y:crop_y+crop_h, crop_x:crop_x+crop_w]\n",
    "    img_ff_BFP = img_ff_BFP / np.mean(img_ff_BFP)\n",
    "\n",
    "    img_ff_GFP = imread(path_ff_base + 'GFP.tiff')[crop_y:crop_y+crop_h, crop_x:crop_x+crop_w]\n",
    "    img_ff_GFP = img_ff_GFP / np.mean(img_ff_GFP)\n",
    "\n",
    "    img_ff_mCh = imread(path_ff_base + 'mCherry.tiff')[crop_y:crop_y+crop_h, crop_x:crop_x+crop_w]\n",
    "    img_ff_mCh = img_ff_mCh / np.mean(img_ff_mCh)\n",
    "\n",
    "    img_df = imread(path_ff_base + 'dark.tiff')[crop_y:crop_y+crop_h, crop_x:crop_x+crop_w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and parse ff and df images\n",
    "if flatfield:\n",
    "    if not ff_custom:\n",
    "        img_ff_BFP = imread(path_ff_BFP)\n",
    "        img_ff_GFP = imread(path_ff_GFP)\n",
    "        img_ff_mCh = imread(path_ff_mCh)\n",
    "        img_ff_TMR = imread(path_ff_TMR)\n",
    "        img_ff_Cy5 = imread(path_ff_Cy5)    \n",
    "        img_df = imread(path_df).astype(np.uint16)\n",
    "\n",
    "    # HERE SPECIFY ff channels to use, consistent to channel list specified above\n",
    "    # (reasonable default channel setup assumed here)\n",
    "    if len(channels) == 3:\n",
    "        img_ff = np.stack([img_ff_mCh, img_ff_GFP, img_ff_BFP])\n",
    "    if len(channels) == 2:\n",
    "        img_ff = np.stack([img_ff_GFP, img_ff_BFP])\n",
    "\n",
    "    if (not img_ff[0].shape == img_df.shape == data_img.shape[-2:]) or (not sizes.get('C', 1) == img_ff.shape[0]):\n",
    "        # check if dim X and Y are same as ff/df X and Y, and same number of images provides as channels\n",
    "        print(\"Wrong dimensions, fix DF and FF, for now no FF correction!\")\n",
    "        flatfield = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show example\n",
    "plt.imshow(img_ff_GFP)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stitching\n",
    "Two options:\n",
    "a. Stage coordinates hard paste\n",
    "b. MIST compute optimal stitching using image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find tile region(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-axis flip for some microscopes\n",
    "if (flip_x):\n",
    "    positions_first['X'] = positions_first['X'] * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_init, overlap_percent = TileRegion.find_tile_region_start(sizes, px_size, positions)\n",
    "print(tr_init)\n",
    "print(overlap_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot stage positions\n",
    "plt.scatter(positions_first['X'], positions_first['Y'], c = positions_first.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_tr_all = TileRegion.find_tile_region_all(tr_init, sizes, positions_first, px_size)\n",
    "print(positions_tr_all.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Well codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export well code per TR (don't run if you have Meghan data)\n",
    "tileregion_names = positions_tr_all.merge(positions_first[['iP', 'name']], left_on = 'P', right_on = 'iP').groupby('TR').head(1)\n",
    "tileregion_names['TR_name'] = tileregion_names['name'].str.split('_').str.get(0)\n",
    "\n",
    "tileregion_names[['TR', 'TR_name']].to_csv(path_export_stitch + \"/\" + name + \"_TRNames.csv\", sep = \";\", decimal = \".\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Place tiles by stage coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions_first_join = pd.merge(positions_tr_all,\n",
    "                                positions_first.rename(columns={'iP': 'P'}),\n",
    "                                how = 'inner', on = 'P')\n",
    "\n",
    "positions_join = TileRegion.perform_stitching_stage(tr_init, positions_first_join, px_size)\n",
    "positions_join.to_csv(path_export_stitch + \"/\" + name + \"_TRLayout.csv\", sep = \";\", decimal = \".\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tileregion result\n",
    "TileRegion.plot_tileregion_layout(positions_join, positions_first, tr_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Compute optimal tile position by MIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp export tile images to tiff\n",
    "TileRegion.export_tile_img_for_stitching(data_img, name, path_export_tile, sizes,\n",
    "                                         tr_init, positions_tr_all, channelsOI_stitch, t_stitch,\n",
    "                                         flatfield, img_ff, img_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagej init\n",
    "ij_mist = imagej.init(path_imagej_mist, add_legacy = True)\n",
    "TileRegion.perform_stitching_mist(ij_mist, name, path_export_tile, tr_init, positions_tr_all, overlap_percent, px_size)\n",
    "# release memory from virtual java/imagej instance!\n",
    "ij_trac.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve MIST result(s)\n",
    "positions_join = TileRegion.parse_mist_result(name, path_export_tile, path_export_stitch, tr_init, positions_tr_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tileregion result\n",
    "TileRegion.plot_tileregion_layout(positions_join, positions_first, tr_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create stitch image from tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tile positions table\n",
    "if 'positions_join' not in locals:\n",
    "    positions_join = pd.read_csv(path_export_stitch + \"/\" + name + \"_TRLayout.csv\", sep = \";\", decimal = \".\")\n",
    "print(positions_join.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitched_images = TileRegion.stitch_tile_images(data_img, tr_init, sizes, positions_join, flatfield, img_ff, img_df, verbose = False, blend = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the stitched images individually\n",
    "for i, arr in enumerate(stitched_images):\n",
    "    np.save(path_export_stitch + name + \"_TR\" + str(i) + \"_img_stitched\", arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation (single cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load calculated TR layout (if not in current session)\n",
    "if 'positions_join' not in locals:\n",
    "    positions_join = pd.read_csv(path_export_stitch + \"/\" + name + \"_TRLayout.csv\", sep = \";\", decimal = \".\")\n",
    "    tr_init = positions_join['TR'].unique()\n",
    "print(positions_join.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stitched image data (if not in current session)\n",
    "if 'stitched_images' not in locals:\n",
    "    stitched_images = []\n",
    "    for tr in tqdm(range(len(tr_init))):\n",
    "        stitched_images.append(np.load(path_export_stitch + name + \"_TR\" + str(tr) + \"_img_stitched.npy\", mmap_mode='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exec nuclear segmentation\n",
    "cellpose_model_nucl = SegmentationCellpose.load_model_cellpose(name = 'nuclei_denoise')\n",
    "\n",
    "results_cellpose_nucl = LoopDimensions.loop_tileregion_np(\n",
    "    img = stitched_images,\n",
    "    func = SegmentationCellpose.create_mask,\n",
    "    tr_init = tr_init,\n",
    "    channels_oi = [channelsOI_cellpose_nucl],\n",
    "    sizes = sizes,\n",
    "    cellpose_model = cellpose_model_nucl,\n",
    "    resample = False)\n",
    "\n",
    "for i, arr in enumerate(results_cellpose_nucl):\n",
    "    np.save(path_analysis_segmentation + name + \"_TR\" + str(i) + \"_mask_nucl\", arr)\n",
    "    # residual save duplicate to tiff for TrackMate import in ImageJ\n",
    "    imwrite(path_analysis_segmentation + name + \"_TR\" + str(i) + \"_mask_nucl.tif\", data = arr.astype('uint16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exec cellular/cytoplasm segmentation on nuclear and cyto channel\n",
    "if channelsOI_cellpose_cell is not None:\n",
    "    cellpose_model_cell = SegmentationCellpose.load_model_cellpose(name = 'cyto3_denoise')\n",
    "\n",
    "    results_cellpose_cell = LoopDimensions.loop_tileregion_np(\n",
    "        img = stitched_images,\n",
    "        func = SegmentationCellpose.create_mask,\n",
    "        tr_init = tr_init,\n",
    "        channels_oi = [channelsOI_cellpose_cell, channelsOI_cellpose_nucl],\n",
    "        sizes = sizes,\n",
    "        channels = [1, 2],\n",
    "        diam = 180,\n",
    "        cellpose_model = cellpose_model_cell,\n",
    "        resample = False)\n",
    "\n",
    "    for i, arr in enumerate(results_cellpose_cell):\n",
    "        np.save(path_analysis_segmentation + name + \"_TR\" + str(i) + \"_mask_cell\", arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking (nuclear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. TrackMate\n",
    "! you probably have to restart the kernel here due to a bug in pyimagej with multiple imagej environments conflicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IJ init (might error first exec)\n",
    "ij_trac = imagej.init([\n",
    "    'net.imagej:imagej:2.5.0',\n",
    "    'sc.fiji:TrackMate:7.9.2',\n",
    "    'sc.fiji:Feature_Detection:2.0.3',\n",
    "    'ome:bioformats_package:6.11.0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking\n",
    "script = open(path_script_Trackmate).read()\n",
    "for file in tqdm([ f for f in os.listdir(path_analysis_segmentation) if (str(f))[-3:] == \"tif\"]):\n",
    "    ij_trac.py.run_script(\"python\", script, {\"path_mask\": path_analysis_segmentation + file, \"path_out\": path_analysis_tracks, \"batchmode\": True})\n",
    "# release memory from virtual java/imagej instance!\n",
    "ij_trac.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. TrackAstra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# trackastra\n",
    "from trackastra.utils import normalize\n",
    "from trackastra.model import Trackastra\n",
    "from trackastra.tracking import graph_to_napari_tracks, graph_to_ctc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load calculated TR layout\n",
    "if 'positions_join' not in locals():\n",
    "    positions_join = pd.read_csv(path_export_stitch + \"/\" + name + \"_TRLayout.csv\", sep = \";\", decimal = \".\")\n",
    "    tr_init = positions_join['tr'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stitched image data (if not in current session)\n",
    "if 'stitched_images' not in locals():\n",
    "    stitched_images = []\n",
    "    for tr in tqdm(range(len(tr_init))):\n",
    "        stitched_images.append(np.load(path_export_stitch + name + \"_TR\" + str(tr) + \"_img_stitched.npy\", mmap_mode='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stitched image data (if not in current session)\n",
    "if 'results_cellpose_nucl' not in locals():\n",
    "    results_cellpose_nucl = []\n",
    "    for tr in tqdm(range(len(tr_init))):\n",
    "        results_cellpose_nucl.append(np.load(path_analysis_segmentation + name + \"_TR\" + str(tr) + \"_mask_nucl.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in tqdm(range(len(tr_init))):\n",
    "    img = stitched_images[tr][:, 0, channelsOI_cellpose_nucl]\n",
    "    labels = results_cellpose_nucl[tr].squeeze()\n",
    "    \n",
    "    # Normalize your images\n",
    "    img_norm = np.stack([normalize(x) for x in img])\n",
    "\n",
    "    # Load a pretrained model\n",
    "    model = Trackastra.from_pretrained(\"general_2d\", device=device)\n",
    "\n",
    "    # Track the cells\n",
    "    track_graph = model.track(img_norm, labels, mode=\"greedy\")  # or mode=\"ilp\", or \"greedy_nodiv\"\n",
    "    \n",
    "    # Convert track graph to tables (Visualise in napari)\n",
    "    napari_tracks, napari_tracks_graph, _ = graph_to_napari_tracks(track_graph)\n",
    "    \n",
    "    # fetch object id from mask for tracks df\n",
    "    maskid_df = []\n",
    "    for frame, mask in enumerate(labels):\n",
    "        subset = napari_tracks[:, 1] == frame\n",
    "        coords = napari_tracks[subset, 2:4]\n",
    "        indexes = np.where(subset)\n",
    "        ids = LinkPointToObject.PointToMaskID(coords, mask)\n",
    "        df = pd.DataFrame({'mask_id': ids, 'index': indexes[0]}, columns=['mask_id', 'index'])\n",
    "        maskid_df.append(df)\n",
    "\n",
    "    maskid_df = pd.concat(maskid_df)\n",
    "    maskid_df.set_index('index', inplace = True)\n",
    "    track_df = pd.DataFrame(napari_tracks, columns=['track_id', 'frame', 'y', 'x']).join(maskid_df)\n",
    "\n",
    "    # write to file\n",
    "    track_df.to_csv(path_analysis_tracks + name + \"_TR\" + str(tr) + \"_tracks_TrackAstra.csv\", index = False)\n",
    "\n",
    "    print(track_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot detection\n",
    "! Executed on single tile images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SpotSpotiflow\n",
    "importlib.reload(SpotSpotiflow)\n",
    "\n",
    "name_model_spotiflow = 'general' # published model\n",
    "model_spotiflow = SpotSpotiflow.load_model(name_model_spotiflow)\n",
    "results_spotiflow = LoopDimensions.loop_tiles_pd(data_img = data_img, sizes = sizes,\n",
    "                                                 func = SpotSpotiflow.spot_detection_pd,\n",
    "                                                 model = model_spotiflow,\n",
    "                                                 channels_oi = channelsOI_spots,\n",
    "                                                 dict_flatfield = {'flatfield': flatfield,\n",
    "                                                                   'flatfield_func': FlatDarkField.ffdf,\n",
    "                                                                   'img_ff': img_ff, 'img_df': img_df\n",
    "                                                 })\n",
    "\n",
    "results_spotiflow.to_csv(path_or_buf = path_analysis_spots + name + \"_all_spots_spotiflow_\" + name_model_spotiflow + \".csv\", sep = \";\", decimal = \".\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell representation tile overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load calculated TR layout (if not in current session)\n",
    "if 'positions_join' not in locals():\n",
    "    positions_join = pd.read_csv(path_export_stitch + \"/\" + name + \"_TRLayout.csv\", sep = \";\", decimal = \".\")\n",
    "    tr_init = positions_join['TR'].unique()\n",
    "print(positions_join.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load segmentation maps (if not in current session)\n",
    "if 'results_cellpose_nucl' not in locals:\n",
    "    results_cellpose_nucl = []\n",
    "    for i, arr in enumerate(tr_init):\n",
    "        results_cellpose_nucl.append(np.load(path_analysis_segmentation + name + \"_TR\" + str(i) + \"_mask_nucl.npy\"))\n",
    "        print(results_cellpose_nucl[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify in what cells are presented most completely in which tiles\n",
    "object_overlap_max = []\n",
    "for TR, TR_start in enumerate(tqdm(tr_init)):\n",
    "    results_cellpose_TR = results_cellpose_nucl[TR].squeeze()\n",
    "    positions_join_TR = positions_join[positions_join['TR'] == TR]\n",
    "    #print(results_cellpose_TR.shape)\n",
    "    for i_slice, mask_slice in enumerate(tqdm(results_cellpose_TR)):\n",
    "        # Calculate the area of each object in mask of stitched image\n",
    "        n_obj = mask_slice.max()\n",
    "        props_TR = measure.regionprops_table(mask_slice, properties = ['label', 'area'])\n",
    "        \n",
    "        # initiate empty array for storing values\n",
    "        object_overlap_current = np.zeros(shape = (n_obj, positions_join_TR.shape[0]), dtype = np.uint16)\n",
    "        \n",
    "        # Calculate area for each object in mask of each tile (crop from stitch)\n",
    "        for index, row in positions_join_TR.iterrows():\n",
    "            x_px_start = row['x_px']\n",
    "            y_px_start = row['y_px']\n",
    "            \n",
    "            # crop mask\n",
    "            mask_slice_tile = mask_slice[y_px_start : y_px_start + sizes['Y'], x_px_start : x_px_start + sizes['X']]\n",
    "\n",
    "            # calculate area of each object in tile\n",
    "            props_tile = measure.regionprops_table(mask_slice_tile, properties = ['label', 'area'])\n",
    "            object_overlap_current[props_tile['label']-1, row['index']] = props_tile['area'].astype(np.uint16)\n",
    "        \n",
    "        # over all FOVs (stored in cols) determine where area representation for each object is largest\n",
    "        object_overlap_current_max = object_overlap_current.argmax(axis = 1)\n",
    "        # calculate fraction of overlap relative to stitched mask\n",
    "        object_overlap_current_max_fraction = np.amax(object_overlap_current, axis = 1) / props_TR['area']\n",
    "\n",
    "        # translate TR index to P in whole dataset\n",
    "        object_overlap_current_max_join = pd.merge(positions_join_TR, pd.DataFrame({'index': object_overlap_current.argmax(axis = 1), 'overlap': object_overlap_current_max_fraction, 'IDCell': range(1, n_obj+1)}), on = 'index')\n",
    "        \n",
    "        # combine and append data\n",
    "        object_overlap_current_max_df = pd.DataFrame({\n",
    "            'IDCell': object_overlap_current_max_join['IDCell'],\n",
    "            'position_max': object_overlap_current_max_join['P'],\n",
    "            'T': i_slice,\n",
    "            'TR': TR,\n",
    "            'overlap': object_overlap_current_max_join['overlap'],\n",
    "        })\n",
    "        object_overlap_max.append(object_overlap_current_max_df)\n",
    "object_overlap_max = pd.concat(object_overlap_max, axis = 0)\n",
    "\n",
    "object_overlap_max.to_csv(path_or_buf = path_analysis_segmentation + name + \"_mask_tile_overlap.csv\", sep = \";\", decimal = \".\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview result\n",
    "print(object_overlap_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot, here you expect mostly random/even distribution\n",
    "plt.hist(object_overlap_max['position_max'], bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot, here you expect right skewed data (cells are mostly found with 100% overlap in discete tiles versus stitch)\n",
    "plt.hist(object_overlap_max['overlap'], bins = 100)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot association & filtering\n",
    "Bind spots to segmented objects (nuclei) & optimal object overlap with tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load calculated TR layout (if not in current session)\n",
    "if 'positions_join' not in locals():\n",
    "    positions_join = pd.read_csv(path_export_stitch + name + \"_TRLayout.csv\", sep = \";\", decimal = \".\")\n",
    "    tr_init = positions_join['TR'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load segmentation data (if not in current session)\n",
    "if 'results_cellpose_nucl' not in locals():\n",
    "    results_cellpose_nucl = []\n",
    "    for i, arr in enumerate(tr_init):\n",
    "        results_cellpose_nucl.append(np.load(path_analysis_segmentation + name + \"_TR\" + str(i) + \"_mask_nucl.npy\"))\n",
    "        print(results_cellpose_nucl[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spots data (if not in current session)\n",
    "if 'results_spotiflow' not in locals():\n",
    "    results_spotiflow = pd.read_csv(path_analysis_spots + name + \"_all_spots_spotiflow_general.csv\", sep = \";\", decimal = \".\")\n",
    "    name_source_spots = 'spotiflow_general'\n",
    "results_spots = results_spotiflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load object tile overlap data (if not in current session)\n",
    "if 'object_overlap_max' not in locals():\n",
    "    object_overlap_max = pd.read_csv(path_analysis_segmentation + name + \"_mask_tile_overlap.csv\", sep = \";\", decimal = \".\")\n",
    "print(object_overlap_max.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad spot XY by tile coordinates by tile position\n",
    "results_spots_pad = results_spots.merge(positions_join, on = 'P')\n",
    "results_spots_pad['X_padded'] = results_spots_pad['X'] + results_spots_pad['x_px']\n",
    "results_spots_pad['Y_padded'] = results_spots_pad['Y'] + results_spots_pad['y_px']\n",
    "results_spots_pad['T'] = results_spots_pad['T'].astype(int)\n",
    "results_spots_pad['C'] = results_spots_pad['C'].astype(int)\n",
    "results_spots_pad['P'] = results_spots_pad['P'].astype(int)\n",
    "results_spots_pad.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find segmentation mask object ID for all spots\n",
    "maskIDs = []\n",
    "\n",
    "for TR, _ in enumerate(tqdm(tr_init)):\n",
    "    # for each TR\n",
    "    for slice, mask in enumerate(tqdm(results_cellpose_nucl[TR].squeeze())):\n",
    "        # for each timeslice\n",
    "        spots = results_spots_pad[(results_spots_pad['T'] == slice) & (results_spots_pad['TR'] == TR)][['Y_padded','X_padded']]\n",
    "        maskID = LinkPointToObject.PointToMaskID(spots.values, mask)\n",
    "        maskID = np.column_stack((maskID, spots.index))\n",
    "        maskIDs.append(maskID)\n",
    "maskIDs = pd.DataFrame(np.concatenate(maskIDs), columns = ['ID', 'index']).set_index('index')\n",
    "\n",
    "# bind mask id to spot dataframe\n",
    "results_spots_assigned = results_spots_pad.join(maskIDs)\n",
    "\n",
    "results_spots_assigned.to_csv(path_or_buf = path_analysis_spots + name + \"_all_spots_assigned_\" + name_source_spots + \".csv\", sep=\";\", decimal = \".\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_spots_assigned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many spots are now not assigned to cells?\n",
    "spots_unmatched_n = (results_spots_assigned['ID'] == 0).value_counts()\n",
    "print(spots_unmatched_n)\n",
    "print(results_spots_assigned['P'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with max object tile overlap\n",
    "print(results_spots_assigned.shape)\n",
    "results_spots_assigned_filter = results_spots_assigned.merge(object_overlap_max, left_on = ['T', 'P', 'ID', 'TR'], right_on = ['T', 'position_max', 'IDCell', 'TR'], how = 'inner')\n",
    "results_spots_assigned_filter = results_spots_assigned_filter.drop(['position_max', 'overlap'], axis = 1)\n",
    "print(results_spots_assigned_filter.shape)\n",
    "\n",
    "results_spots_assigned_filter.to_csv(path_or_buf = path_analysis_spots + name + \"_all_spots_assigned_filter_\" + name_source_spots + \".csv\", sep = \";\", decimal = \".\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_spots_assigned_filter.head())\n",
    "print(results_spots_assigned_filter['P'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction single-cell (morphology & intensity)\n",
    "! Executed on single tile images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(FeatureExtraction)\n",
    "keys = list(dict.keys(sizes))\n",
    "\n",
    "results_features = []\n",
    "for inds in product(*map(range, data_img.blocks.shape)):\n",
    "    # fetch current position in dataset=\n",
    "    i_T = inds[keys.index('T')]\n",
    "    i_P = inds[keys.index('P')]\n",
    "    XY_px = positions_join.loc[positions_join['P'] == i_P, ['y_px', 'x_px']].values.squeeze()\n",
    "    TR = positions_join.loc[positions_join['P'] == i_P, 'TR'].values.squeeze() #0\n",
    "    \n",
    "    chunk = data_img.blocks[inds].squeeze()\n",
    "    # crop mask to tile\n",
    "    mask_TR = results_cellpose_nucl[TR].squeeze()\n",
    "    mask = mask_TR[i_T, XY_px[0]: XY_px[0] + sizes['Y'], XY_px[1]: XY_px[1] + sizes['X']]\n",
    "    \n",
    "    if chunk.ndim == 3:\n",
    "        img = chunk.compute().squeeze()\n",
    "        if flatfield:\n",
    "            for C in range(img.shape[0]):\n",
    "                img = img.astype(np.float32)\n",
    "                img[C] = FlatDarkField.ffdf(img[C], img_ff[C].squeeze(), img_df)\n",
    "        img = np.transpose(img, axes=(1, 2, 0))\n",
    "\n",
    "    if features_extra_percentile_subtract:\n",
    "        img_mod = np.zeros_like(img)\n",
    "        for iC, name_channel in enumerate(channels):\n",
    "            img_mod[:,:,iC] = im.img_object_percentile_subtract(img[:,:,iC], mask, percent=20)\n",
    "        img = np.concatenate((img, img_mod), axis = -1)\n",
    "\n",
    "    data_features = FeatureExtraction.extract_intensity_features_img(img, mask)\n",
    "    \n",
    "    # TODO bind channel names (and mod name) to colnames, instead of -0/-1 suffixes\n",
    "    # print(data_features.columns)\n",
    "    \n",
    "    data_features['P'] = i_P\n",
    "    data_features['T'] = i_T\n",
    "    data_features['TR'] = TR\n",
    "    results_features.append(data_features)\n",
    "results_features = pd.concat(results_features)\n",
    "\n",
    "# store to file\n",
    "results_features.to_csv(path_features + name + \"_features_cell.csv\", index = False)\n",
    "results_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join with max object tile overlap\n",
    "results_features_filter = results_features.merge(object_overlap_max, left_on = ['T', 'P', 'label', 'TR'], right_on = ['T', 'position_max', 'IDCell', 'TR'], how='inner')\n",
    "results_features_filter = results_features_filter.drop(['position_max'], axis = 1)\n",
    "\n",
    "results_features_filter.to_csv(path_features + name + \"_features_cell_filter.csv\", index = False)\n",
    "results_features_filter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization (Napari)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'px_size' not in locals:\n",
    "    px_size = MetadataParserND2.get_nd2_pxsize(path_nd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load calculated TR layout (if not in current session)\n",
    "if 'positions_join' not in locals:\n",
    "    positions_join = pd.read_csv(path_export_stitch + \"/\" + name + \"_TRLayout.csv\", sep = \";\", decimal = \".\")\n",
    "    tr_init = positions_join['TR'].unique()\n",
    "print(positions_join.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stitched image data (if not in current session)\n",
    "if 'stitched_images' not in locals:\n",
    "    stitched_images = []\n",
    "    for tr in tqdm(range(len(tr_init))):\n",
    "        stitched_images.append(np.load(path_export_stitch + name + \"_TR\" + str(tr) + \"_img_stitched.npy\")) #, mmap_mode='r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load segmentation data (if not in current session)\n",
    "if 'results_cellpose_nucl' not in locals():\n",
    "    results_cellpose_nucl = []\n",
    "    for i, arr in enumerate(tr_init):\n",
    "        results_cellpose_nucl.append(np.load(path_analysis_segmentation + name + \"_TR\" + str(i) + \"_mask_nucl.npy\"))\n",
    "        print(results_cellpose_nucl[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.dims.axis_labels = ['T', 'Y', 'X']\n",
    "viewer.scale_bar.visible = True\n",
    "viewer.scale_bar.font_size = 20\n",
    "viewer.scale_bar.unit = \"um\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image stitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which tile-region image to take from the set?\n",
    "tr = 0\n",
    "# fetch\n",
    "stitched_image = stitched_images[tr]\n",
    "stitched_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stitch to napari\n",
    "viewer.add_image(data_img,\n",
    "                 channel_axis = 3,\n",
    "                 name = channels,\n",
    "                 blending = 'additive',\n",
    "                 gamma = 1,\n",
    "                 scale = (px_size, px_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch segmentation map\n",
    "result_cellpose_nucl = results_cellpose_nucl[tr]\n",
    "print(result_cellpose_nucl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view segmentation map\n",
    "viewer.add_labels(result_cellpose_nucl,\n",
    "                  scale = (px_size, px_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tracking data, highlight few tracks\n",
    "path_overnight_tracking_all = path_analysis_tracks + name + \"_TR\" + str(n_img) + \"_mask_nucl_all.csv\"\n",
    "\n",
    "data_tracks_csv = pd.read_csv(path_overnight_tracking_all, delimiter = \",\")\n",
    "data_tracks_numpy = data_tracks_csv.loc[:, ['IDTrack', 't', 'y', 'x']].to_numpy()\n",
    "\n",
    "data_tracks_numpy = np.insert(data_tracks_numpy, 2, np.array([0]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view tracks\n",
    "viewer.add_tracks(\n",
    "    data_tracks_numpy,\n",
    "    name = \"nuclei_tracks_all\",\n",
    "    colormap = 'hsv',\n",
    "    tail_length = 20,\n",
    "    scale = (px_size, px_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unassigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spots data (if not in current session)\n",
    "if 'results_spotiflow' not in locals():\n",
    "    results_spotiflow = pd.read_csv(path_analysis_spots + name + \"_all_spots_spotiflow_general.csv\", sep = \";\", decimal = \".\")\n",
    "    name_source_spots = 'spotiflow_general'\n",
    "results_spots = results_spotiflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad spot XY by tile coordinates\n",
    "results_spots_pad = results_spots.merge(positions_join, on = 'P')\n",
    "results_spots_pad['X_padded'] = results_spots_pad['X'] + results_spots_pad['x_px']\n",
    "results_spots_pad['Y_padded'] = results_spots_pad['Y'] + results_spots_pad['y_px']\n",
    "results_spots_pad['T'] = results_spots_pad['T'].astype(int)\n",
    "results_spots_pad['C'] = results_spots_pad['C'].astype(int)\n",
    "results_spots_pad['P'] = results_spots_pad['P'].astype(int)\n",
    "print(results_spots_pad.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize all points\n",
    "viewer.add_points(\n",
    "    np.insert(results_spots_pad[['T', 'Y_padded', 'X_padded']].values, 1, np.array([0]), axis = 1),\n",
    "    name = \"spots_ALL\",\n",
    "    scale = (px_size, px_size),\n",
    "    size = 15,\n",
    "    face_color = '#ffffff00',\n",
    "    edge_color = 'white',\n",
    "    edge_width = 0.1,\n",
    "    visible = True,\n",
    "    ndim = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize all points, color by detection probability\n",
    "data_spots_features = {'prob': results_spots_pad['spot_prob']}\n",
    "face_color_cycle = ['green', 'red']\n",
    "\n",
    "viewer.add_points(\n",
    "    np.insert(results_spots_pad[['T', 'Y_padded', 'X_padded']].values, 1, np.array([0]), axis = 1),\n",
    "    name = \"spots_ALL_color_prob\",\n",
    "    features = data_spots_features,\n",
    "    scale = (px_size, px_size),\n",
    "    size = 15,\n",
    "    face_color = '#ffffff00',\n",
    "    edge_color = 'prob',\n",
    "    edge_color_cycle = face_color_cycle,\n",
    "    edge_width = 0.1,\n",
    "    visible = True,\n",
    "    ndim = 4\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigned & Filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_spots_assigned_filter = pd.read_csv(path_analysis_spots + name + \"_all_spots_assigned_filter_spotiflow_general.csv\", sep = \";\", decimal = \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize nucleus assigned and dedeplicated spots\n",
    "viewer.add_points(\n",
    "    np.insert(results_spots_assigned_filter[results_spots_assigned_filter['TR'] == n_img][['T', 'Y_padded', 'X_padded']].values, 1, np.array([0]), axis = 1),\n",
    "    name = \"spots_ALL\",\n",
    "    scale = (px_size, px_size),\n",
    "    size = 15,\n",
    "    opacity=.2,\n",
    "    face_color = '#ffffff00',\n",
    "    border_color = 'white',\n",
    "    border_width = 0.1,\n",
    "    visible = True,\n",
    "    ndim = 4\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "__apptainer__",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
