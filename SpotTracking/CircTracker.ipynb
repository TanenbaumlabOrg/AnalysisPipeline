{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot tracking pipeline\n",
    "This notebook handles a single tiff or nd2 file, with layout [T, Y, X] (single position & timelapse dataset, assumed to be single-channel).\n",
    "\n",
    "## Key pipeline components:\n",
    "- Data loading\n",
    "  - ND2 parsing by [nd2](https://github.com/tlambert03/nd2) or tiff loading (tifffile)\n",
    "  - (optional) flatfield illumination correction\n",
    "- Spot detection by Laplacian of Gaussian [scikit-image](https://github.com/scikit-image/scikit-image)\n",
    "- Spot tracking by [LapTrack](https://github.com/yfukai/laptrack)\n",
    "  - Custom metric, integrating distance & intensity difference\n",
    "- Spot intensity extraction + background subtraction (inspired by [TransTrack](https://github.com/TanenbaumLab/TransTrack))\n",
    "- Track filtering (optional)\n",
    "  - Mask overlap (manual)\n",
    "  - Track feature treshold\n",
    "    - Length\n",
    "    - Spot density\n",
    "    - Start window\n",
    "  - Manual toggle\n",
    "- Data visualization with Napari\n",
    "- Spot track isolation (crops) focussed on track ends\n",
    "  - Annotation\n",
    "\n",
    "- Batch mode for all automated steps (loop over image files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default libs\n",
    "import os, sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import reduce\n",
    "\n",
    "# Papallel processing\n",
    "import ray\n",
    "from ray.util.multiprocessing import Pool\n",
    "pool = Pool()\n",
    "\n",
    "# Data loading\n",
    "from imageio import imread_v2\n",
    "import nd2\n",
    "\n",
    "# Vizualisation\n",
    "import napari\n",
    "import matplotlib.pyplot as plt\n",
    "import distinctipy\n",
    "\n",
    "# Detection and tracking\n",
    "from skimage.feature import blob_log\n",
    "from laptrack import LapTrack\n",
    "    \n",
    "# Measurements\n",
    "from skimage import measure\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.spatial.distance import cdist\n",
    "from skimage.segmentation import expand_labels\n",
    "\n",
    "# find path to function imports\n",
    "from pathlib import Path\n",
    "path_imports = str(Path(os.getcwd()).resolve().parents[0]) + '/src/'\n",
    "sys.path.append(path_imports)\n",
    "\n",
    "# import external function\n",
    "import importlib\n",
    "import FlatDarkField\n",
    "import SpotGeneric\n",
    "import TrackFilter\n",
    "import LinkPointToObject\n",
    "import Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image load tiff\n",
    "def load_image(path, flatfield, img_ff, img_df):\n",
    "    '''\n",
    "    Loads image (tiff or nd2, all slices) and applies flatfield correction.\n",
    "    '''\n",
    "    filename = os.path.basename(path)\n",
    "    extension = filename.split('.')[-1]\n",
    "\n",
    "    if extension.startswith('tif'):\n",
    "        img = imread_v2(path)\n",
    "    elif extension == 'nd2':\n",
    "        img = nd2.imread(path)\n",
    "    else:\n",
    "        raise ValueError('Invalid file extension')\n",
    "\n",
    "    if flatfield:\n",
    "        img = FlatDarkField.ffdf_series(img, img_ff, img_df)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_time_window_below_per_group(df, group_col, value_col, window):\n",
    "    grouped = df.groupby(group_col)\n",
    "    filtered_df = pd.DataFrame()\n",
    "\n",
    "    for _, group in grouped:\n",
    "        max_value = group[value_col].max()\n",
    "        threshold = max_value - window\n",
    "        mask = group[value_col] >= threshold\n",
    "        # print(max_value, threshold, sum(mask))\n",
    "        filtered_group = group[mask]\n",
    "        filtered_df = pd.concat([filtered_df, filtered_group])\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = '/dummy/'\n",
    "\n",
    "path_analysis_spots = path_base + \"/Spots/\"\n",
    "path_analysis_tracks = path_base + \"/Tracks/\"\n",
    "path_features = path_base + \"/Features/\"\n",
    "path_figures = path_base + \"/Figures/\"\n",
    "path_movies = path_base + \"/Movies/\"\n",
    "\n",
    "os.makedirs(path_analysis_spots, exist_ok=True)\n",
    "os.makedirs(path_analysis_tracks, exist_ok=True)\n",
    "os.makedirs(path_features, exist_ok=True)\n",
    "os.makedirs(path_figures, exist_ok=True)\n",
    "os.makedirs(path_movies, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should flatfield correction be executed?\n",
    "flatfield = False #True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection settings\n",
    "detection_threshold = 10**-3 #with flatfield corrected data 10**-3, without correction typically 30-75\n",
    "measurement_radius = 4 #in px\n",
    "\n",
    "# Tracking settings\n",
    "distance_threshold = 20 #in px\n",
    "frame_gap_frames = 2 #number of frames no spot to be tracked\n",
    "frame_gap_distance = 20 #in px\n",
    "max_relIntDiff = .5 # intensity gap to break track linking\n",
    "\n",
    "# Output filtering & arrangement\n",
    "track_length_minimum = 3\n",
    "track_density_maximum = 1200\n",
    "threshold_include_max_outside = .1\n",
    "frame_threshold = 10 # track start window\n",
    "retain_before_frame = True # before or after frame number\n",
    "\n",
    "track_index_rearange_length = True #(export only)\n",
    "\n",
    "# Measurement settings\n",
    "bbox_extent = 8 # size (width & height) of intensity extraction box, scale this with pixel size and spot size in images\n",
    "coef_var_thr = .4 # coeficient of variation theshold for inclusion of background measurement boxes surrounding spot detections. Better be a little stringent here! (based on histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flat field definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ff and df images\n",
    "path_ff = \"G:/Group Tanenbaum/Lab Microscopy/misc/Flat field correction images/Flatfieldimages_Harry_20230608/\"\n",
    "path_ff_GFP = path_ff + \"488_8-6-2023_normalizedflatfieldimage_100ms.tif\"\n",
    "path_df = path_ff + \"darkfieldimage__100ms.tif\"\n",
    "\n",
    "if flatfield:\n",
    "    img_ff_GFP = imread_v2(path_ff_GFP).squeeze()\n",
    "    img_df = np.round(imread_v2(path_df)).astype(np.uint16)\n",
    "    img_ff = img_ff_GFP\n",
    "else:\n",
    "    img_ff = []\n",
    "    img_df = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading (nd2 or tiff)\n",
    "! a single file/dataset is assumed to be single position and single channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list image files in directory\n",
    "img_files = [i for i in os.listdir(path_base) if i.endswith('.tif') or i.endswith('.nd2')]\n",
    "print(img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select image region to analyze\n",
    "filename = img_files[0]\n",
    "path_img = path_base + filename\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_image(path_img, flatfield, img_ff, img_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate a viewer\n",
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image into viewer\n",
    "viewer.add_image(img, name = 'image', blending = 'additive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute\n",
    "spots = SpotGeneric.detect_spots(img = img, thresholdLoG = detection_threshold)\n",
    "print(len(spots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot spot number per movie frame\n",
    "list_n_spots = []\n",
    "for i in range(img.shape[0]):\n",
    "    n_spots = sum(spots[:,0] == i)\n",
    "    list_n_spots.append(n_spots)\n",
    "\n",
    "plt.figure(figsize = (5, 2))\n",
    "plt.plot(list_n_spots)\n",
    "plt.title('n-spots per frame')\n",
    "plt.xlabel('frame')\n",
    "plt.ylabel('count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add spots to viewer\n",
    "viewer.add_points(spots, name = 'Spots', size = 10, face_color = 'transparent', border_color = 'red', border_width = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot feature extraction\n",
    "Used for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spots, spots_int = SpotGeneric.measure_spots_intensities(spots = spots, radius = measurement_radius, img = img)\n",
    "spots_int_rel, spots_in_range = SpotGeneric.metric_spots_relative_intensity(spots = spots, spotsIntensity = spots_int, gapMax = frame_gap_frames)\n",
    "\n",
    "coordinate_df = pd.DataFrame(spots, columns = ['frame', 'centroid0', 'centroid1'])\n",
    "\n",
    "coordinate_df['label'] = np.arange(len(spots))\n",
    "coordinate_df['frame'] = coordinate_df['frame'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking\n",
    "Tracking metric = eucledian distance * (1 + absolute relative intensity difference)\n",
    "\n",
    "A = mean intensity parent  \n",
    "B = mean intensity link candidate  \n",
    "\n",
    "|A - B| / A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laptrack custom tracking metric\n",
    "def LAP_link_metric_intensity(c1, c2):\n",
    "    (frame1, label1), (frame2, label2) = c1, c2\n",
    "    if frame1 > frame2:\n",
    "        # switch spots in query if time order is wrong\n",
    "        tmp = (frame1, label1)\n",
    "        (frame1, label1) = (frame2, label2)\n",
    "        (frame2, label2) = tmp\n",
    "\n",
    "    # double frame index\n",
    "    ind = (frame1, frame2, label1, label2)\n",
    "    if ind in spots_int_rel.index:\n",
    "        if spots_int_rel.loc[ind][\"relIntDiff\"] >= max_relIntDiff:\n",
    "            # Too large difference intensity\n",
    "            value = np.float32(\"inf\")\n",
    "        else:\n",
    "            # Valid result\n",
    "            value = spots_int_rel.loc[ind][\"distanceScaled\"]\n",
    "    else:\n",
    "        # No candidates found\n",
    "        value = np.float32(\"inf\")\n",
    "    return value\n",
    "\n",
    "lap_tracker = LapTrack(\n",
    "    track_dist_metric = LAP_link_metric_intensity,\n",
    "    track_cost_cutoff = distance_threshold,\n",
    "    gap_closing_dist_metric =  LAP_link_metric_intensity,\n",
    "    gap_closing_max_frame_count = frame_gap_frames-2,\n",
    "    gap_closing_cost_cutoff = frame_gap_distance,\n",
    "    splitting_cost_cutoff = False,\n",
    "    merging_cost_cutoff = False,\n",
    "    parallel_backend = 'ray'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute\n",
    "track_df, split_df, merge_df = lap_tracker.predict_dataframe(\n",
    "    coordinate_df, coordinate_cols = [\"frame\", \"label\"], only_coordinate_cols = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot track starts and ends\n",
    "track_starts = track_df.groupby('track_id')['frame_y'].min().values\n",
    "track_ends = track_df.groupby('track_id')['frame_y'].max().values\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 3))\n",
    "ax1.set_title('track starting frame')\n",
    "ax1.hist(track_starts, bins=50, label='starts')\n",
    "ax1.set_xlabel('frame')\n",
    "ax1.set_ylabel('n tracks')\n",
    "\n",
    "ax2.set_title('track ending frame')\n",
    "ax2.hist(track_ends, bins=50, label='ends')\n",
    "ax2.set_xlabel('frame')\n",
    "ax2.set_ylabel('n tracks')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load track_df to Napari\n",
    "viewer.add_tracks(track_df[[\"track_id\", \"frame_y\", \"centroid0\", \"centroid1\"]],\n",
    "                  tail_length=1, tail_width=4,\n",
    "                  colormap='hsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make points into labelmap, color by track_id\n",
    "mask = np.zeros(img.shape, dtype = np.uint16)\n",
    "\n",
    "# Make point objects in empty mask, index by track_id + 1\n",
    "for i, row in track_df.iterrows():\n",
    "    mask[(int(row['frame_y']), int(row['centroid0']), int(row['centroid1']))] = row['track_id'] + 1\n",
    "\n",
    "# Expand individual labels in the mask, this method enables closeby points to have non-overlapping fill areas\n",
    "expanded_labels = list()\n",
    "for i in range(0,len(mask)):\n",
    "    expanded_labels.append(expand_labels(mask[i], distance = 5))\n",
    "expanded_labels = np.array(expanded_labels)\n",
    "\n",
    "viewer.add_labels(expanded_labels, name = 'Spots by Track_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intensity extraction & background subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract intensities from image frames\n",
    "tracks = track_df.reset_index()\n",
    "data_frames = np.zeros(shape = [len(tracks), 25, bbox_extent**2])\n",
    "\n",
    "list_frames = np.unique(tracks['frame_y'].values)\n",
    "for frame in list_frames:\n",
    "    spots_df = tracks[tracks['frame_y'] == frame]\n",
    "    spots_frame = spots_df[['centroid0', 'centroid1']].values.astype('int')\n",
    "    data_frames[spots_df.index] = SpotGeneric.extract_bbox_grid_intensity(img[frame], spots_frame, bbox_extent)\n",
    "\n",
    "bbox_indices = SpotGeneric.index_bbox_grid_tiers()\n",
    "intensity_peak, intensity_bg, intensity_peaksub, coef = SpotGeneric.parse_bbox_grid_intensity(data_frames, bbox_indices, thr_bg = coef_var_thr)\n",
    "\n",
    "print('n of spots with invalid background:', np.sum(np.isnan(intensity_bg)))\n",
    "\n",
    "plt.hist(coef.flatten(), bins = 100)\n",
    "plt.title('coeficient of variation all bboxes (including spots)')\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('coeficient of variation')\n",
    "plt.axvline(x=coef_var_thr, color='r', linestyle='--') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC figure\n",
    "bins = np.linspace(0, round(np.max(intensity_peak)), 100)\n",
    "\n",
    "plt.figure(figsize = (10, 4))\n",
    "plt.hist(intensity_peak, label = 'peak', bins = bins, alpha = .5)\n",
    "plt.hist(intensity_bg, label = 'bg', bins = bins, alpha = .5)\n",
    "plt.hist(intensity_peaksub, label = 'peak_bg', bins = bins, alpha = .5)\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('intensity value')\n",
    "plt.title('spot & background signal histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join intenstity data to tracking table\n",
    "tracks_join = tracks.join(\n",
    "    pd.DataFrame({\n",
    "        'intensity': intensity_peak,\n",
    "        'background': intensity_bg,\n",
    "        'intensity_bg': intensity_peaksub\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot example background stability\n",
    "unique_track_ids, counts = np.unique(tracks_join['track_id'].values, return_counts=True)\n",
    "largest_group_ids = unique_track_ids[np.argsort(counts)[-5:]]\n",
    "track_sample = tracks_join[np.isin(tracks_join['track_id'].values, largest_group_ids)]\n",
    "\n",
    "for i in largest_group_ids:\n",
    "    sample = track_sample[track_sample['track_id'] == i]\n",
    "    plt.plot(sample['frame'], sample['background'], label = i)\n",
    "plt.title('background intensity stability')\n",
    "plt.xlabel('frame')\n",
    "plt.ylabel('intensity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot longest sample track intensity traces\n",
    "array, counts = np.unique(tracks_join['track_id'].values, return_counts=True)\n",
    "track_sample = tracks_join[tracks_join['track_id'] == array[counts == np.max(counts)][0]]\n",
    "\n",
    "plt.plot(track_sample['frame'], track_sample['background'], label = 'bg')\n",
    "plt.plot(track_sample['frame'], track_sample['intensity'], label = 'int')\n",
    "plt.plot(track_sample['frame'], track_sample['intensity_bg'], label = 'corr')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.xlabel('frame')\n",
    "plt.ylabel('intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC, overlay coef variation values for all bboxes on the images\n",
    "# this draws all bboxes (including spot itself) that are beyond the theshold set\n",
    "\n",
    "img_heatmap = np.zeros_like(img, dtype = coef.dtype)\n",
    "for i, mask_slice in enumerate(img_heatmap):\n",
    "    indexes = tracks_join['frame_y'] == i\n",
    "    spots_current = tracks_join[indexes][['centroid0', 'centroid1']].values.astype('int')\n",
    "    values = coef[np.where(indexes)] >= coef_var_thr #>= coef_var_thr # to show raw values, or excluded bboxes\n",
    "\n",
    "    mask_slice = SpotGeneric.draw_bbox_grid_mask(mask_slice, spots_current, bbox_extent, values, True)\n",
    "\n",
    "viewer.add_image(img_heatmap, name = 'coefVar', colormap = 'inferno', blending = 'additive', opacity = .75, contrast_limits = [0.0, 2.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export all tracks to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_export = tracks_join[['track_id', 'centroid1', 'centroid0', 'frame_y']].copy()\n",
    "\n",
    "df_export.rename(columns={\n",
    "    'centroid1': 'x',\n",
    "    'centroid0': 'y',\n",
    "    'frame_y': 't'\n",
    "}, inplace = True)\n",
    "\n",
    "if track_index_rearange_length:\n",
    "    # re-index track_id value by tracklength\n",
    "    key_counts = df_export['track_id'].value_counts()\n",
    "\n",
    "    # Step 2: Create a rank based on the counts (largest group gets index 1)\n",
    "    rank_mapping = {key: rank + 1 for rank, key in enumerate(key_counts.index)}\n",
    "\n",
    "    # Step 3: Map the new rank to the key column\n",
    "    df_export['track_id'] = df_export['track_id'].map(rank_mapping)\n",
    "\n",
    "df_export.to_csv(path_base + filename + \"_tracks_all.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask-based filtering setup\n",
    "The labels layer will be used for filtering at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_inclusion = viewer.add_labels(name = 'Inclusion', data = np.zeros(img.shape[1:], dtype = bool), blending = 'additive')\n",
    "mask_exclusion = viewer.add_labels(name = 'Exclusion', data = np.zeros(img.shape[1:], dtype = bool), blending = 'additive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Now start drawing mask(s) in the viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inclusion mask filter, tracks inside for 1-'outside threshold' are kept (or all if no mask)\n",
    "try:\n",
    "    mask_inclusion\n",
    "except NameError:\n",
    "    print('no inclusion mask data, anything will be accepted')\n",
    "    tracks_inclusion_cutoff = tracks_join['track_id'].unique()\n",
    "else:\n",
    "    if np.max(mask_inclusion.data):\n",
    "        tracks_inclusion_cutoff = TrackFilter.track_filter_mask(\n",
    "            tracks_join, spots, mask_inclusion.data, threshold=1-threshold_include_max_outside)\n",
    "    else:\n",
    "        print('no mask drawn, but mask exists, anything will be accepted')\n",
    "        tracks_inclusion_cutoff = tracks_join['track_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclusion mask filter, tracks inside for 'outside threshold' are kept (or all if no mask)\n",
    "try:\n",
    "    mask_exclusion\n",
    "except NameError:\n",
    "    print('no exclusion mask data, anything will be accepted')\n",
    "    tracks_exclusion_cutoff = tracks_join['track_id'].unique()\n",
    "else:\n",
    "    if np.max(mask_exclusion.data):\n",
    "        tracks_exclusion_cutoff = TrackFilter.track_filter_mask(\n",
    "            tracks_join, spots, mask_exclusion.data, inside = False, threshold=threshold_include_max_outside)\n",
    "    else:\n",
    "        print('no mask drawn, but mask exists, anything will be accepted')\n",
    "        tracks_exclusion_cutoff = tracks_join['track_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criteria track filtering\n",
    "- Track length (frame number, ! ignores gaps)\n",
    "- Local spot density (quantile-based, time agnostic)\n",
    "- Overlap with inclusion or exclusion mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track length filter (n obs)\n",
    "tracks_length_cutoff = TrackFilter.track_filter_min_length(tracks_join, track_length_minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local spot density per track\n",
    "tracks_density_cutoff = TrackFilter.track_filter_density(tracks_join, spots, track_density_maximum, supplement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracks initiate window filter (before or after frame number)\n",
    "tracks_start_window = TrackFilter.track_filter_start_window(tracks_join, retain_before_frame, frame_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate and parse track_id allow lists\n",
    "tracks_retain = reduce(\n",
    "    np.intersect1d, (\n",
    "        tracks_inclusion_cutoff,\n",
    "        tracks_exclusion_cutoff,\n",
    "        tracks_length_cutoff,\n",
    "        tracks_density_cutoff,\n",
    "        tracks_start_window\n",
    "        )\n",
    "    )\n",
    "\n",
    "print('n tracks to retain: ' + str(len(tracks_retain)))\n",
    "\n",
    "# actual tracking table filter\n",
    "track_df_filter = tracks_join[np.isin(tracks_join['track_id'].values, tracks_retain)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize filtered tracks to napari\n",
    "\n",
    "# Make into labelmap, color by tree_id\n",
    "mask = np.zeros(img.shape, dtype = np.uint16)\n",
    "\n",
    "# Make point objects in empty mask, index by tree_id\n",
    "for i, row in track_df_filter.iterrows():\n",
    "    mask[(int(row['frame_y']), int(row['centroid0']), int(row['centroid1']))] = row['track_id'] + 1\n",
    "\n",
    "# Expand labels in maks\n",
    "expanded_labels = list()\n",
    "for i in range(0,len(mask)):\n",
    "    expanded_labels.append(expand_labels(mask[i], distance = 5))\n",
    "expanded_labels = np.array(expanded_labels)\n",
    "\n",
    "track_labelmap = viewer.add_labels(expanded_labels, name = 'Filtered Tracks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write table to file\n",
    "df_export = track_df_filter[['track_id', 'centroid1', 'centroid0', 'frame_y']].copy()\n",
    "\n",
    "df_export.rename(columns={\n",
    "    'centroid1': 'x',\n",
    "    'centroid0': 'y',\n",
    "    'frame_y': 't'\n",
    "}, inplace = True)\n",
    "\n",
    "if track_index_rearange_length:\n",
    "    # re-index track_id value by tracklength\n",
    "    key_counts = df_export['track_id'].value_counts()\n",
    "\n",
    "    # Step 2: Create a rank based on the counts (largest group gets index 1)\n",
    "    rank_mapping = {key: rank + 1 for rank, key in enumerate(key_counts.index)}\n",
    "\n",
    "    # Step 3: Map the new rank to the key column\n",
    "    df_export['track_id'] = df_export['track_id'].map(rank_mapping)\n",
    "\n",
    "df_export.to_csv(path_base + filename + \"_tracks_filter.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually select tracks for export\n",
    "This will create two layers in the Napari viewer, and allows users to manually select/toggle individual tracks for inclusion in export.\n",
    "- 'track toggle' points layer: All tracks visualized as circles colored by track identity. When this layer is active the user can click individual points to toggle track inclusion.\n",
    "- 'tracks selected' tracks layer: All toggled tracks, starts as an empty layer, the user should not interact with this.\n",
    "\n",
    "! This starts from a tracking table with no filtering applied in the above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init toggle layer\n",
    "colormap = distinctipy.get_colors(70)\n",
    "\n",
    "track_points = viewer.add_points(\n",
    "    data = tracks_join[['frame_y', 'centroid0', 'centroid1']].values,\n",
    "    name = 'track toggle',\n",
    "    features = {'track_id': tracks_join['track_id'].values},\n",
    "    border_color = 'track_id',\n",
    "    face_color = 'transparent',\n",
    "    border_colormap = colormap,\n",
    "    border_width = .15,\n",
    "    size = 10)\n",
    "# track_points.mode = 'select'\n",
    "\n",
    "# empty tracks layes\n",
    "tracks_include = viewer.add_tracks(\n",
    "    data = np.zeros((1,4)),\n",
    "    features = {'track_id': np.zeros((1,))},\n",
    "    name = 'tracks selected',\n",
    "    color_by = 'track_id',\n",
    "    tail_length = 10, tail_width = 3,\n",
    "    colormap = 'hsv')\n",
    "\n",
    "# switch to correct layer\n",
    "viewer.layers.selection.active = track_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init user interactive framework\n",
    "def toggle_track_in_layer(df, idTrack, destination, reindex = True, verbose = False):\n",
    "\tif reindex:\n",
    "\t\tidTrack = idTrack-1\n",
    "\t\n",
    "\tif verbose:\n",
    "\t\tprint(idTrack)\n",
    "\t\tprint('yet present', idTrack in destination.data[:, 0])\n",
    "\t\tprint('valid id', idTrack in df['track_id'].values)\n",
    "\t\tprint(np.unique(destination.data[1:, 0]).astype('int'))\n",
    "\t\n",
    "\t# remove if included\n",
    "\tif idTrack in destination.data[:, 0]:\n",
    "\t\t#print('off')\n",
    "\t\tdestination.data = np.delete(destination.data, np.where(destination.data[:, 0] == idTrack)[0], axis=0)\n",
    "\t\treturn\n",
    "\t\n",
    "\t# add if not included yet\n",
    "\tif idTrack not in destination.data[:, 0] and idTrack in df['track_id'].values:\n",
    "\t\t#print('on')\n",
    "\t\tdestination.data = np.vstack((destination.data, df[df['track_id'] == idTrack][[\"track_id\", \"frame_y\", \"centroid0\", \"centroid1\"]].values))\n",
    "\n",
    "@track_points.mouse_drag_callbacks.append\n",
    "def click(layer, event):\n",
    "\tif event.type == \"mouse_press\":\n",
    "\t\t# is the value passed from the click event?\n",
    "\t\tpoint_index = layer.get_value(\n",
    "\t\t\tevent.position,\n",
    "\t\t\tview_direction=event.view_direction,\n",
    "\t\t\tdims_displayed=event.dims_displayed,\n",
    "\t\t\tworld=True,\n",
    "\t\t)\n",
    "\t\tif point_index is not None:\n",
    "\t\t\tlabel = layer.features['track_id'][point_index]\n",
    "\t\t\ttoggle_track_in_layer(tracks_join, label, tracks_include, reindex\t= False)\n",
    "\n",
    "napari.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Now select the tracks you want to have in the Napari viewer.\n",
    "Have the 'track toggle' layer active and click points.\n",
    "Enabled tracks should become visible in the 'tracks selected' layer with track tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all togled current toggled tracks\n",
    "print(np.unique(tracks_include.data[1:, 0]).astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data for enabled tracks\n",
    "\n",
    "# fetch only tracks that are toggled\n",
    "track_df_toggle = tracks_join[tracks_join['track_id'].isin(np.unique(tracks_include.data[1:, 0]).astype('int'))]\n",
    "\n",
    "# write to file\n",
    "df_export = track_df_toggle[['track_id', 'centroid1', 'centroid0', 'frame_y']].copy()\n",
    "\n",
    "df_export.rename(columns={\n",
    "    'centroid1': 'x',\n",
    "    'centroid0': 'y',\n",
    "    'frame_y': 't'\n",
    "}, inplace = True)\n",
    "\n",
    "if track_index_rearange_length:\n",
    "    # re-index track_id value by tracklength\n",
    "    key_counts = df_export['track_id'].value_counts()\n",
    "\n",
    "    # Step 2: Create a rank based on the counts (largest group gets index 1)\n",
    "    rank_mapping = {key: rank + 1 for rank, key in enumerate(key_counts.index)}\n",
    "\n",
    "    # Step 3: Map the new rank to the key column\n",
    "    df_export['track_id'] = df_export['track_id'].map(rank_mapping)\n",
    "\n",
    "df_export.to_csv(path_base + filename + \"_tracks_toggle.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single track spot cropping, track ends focus\n",
    "Observe the fate of track ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "frame_window = 10 # frames before and after track end to isolate\n",
    "frame_limit = frame_window # track ends how many frames before end of timelapse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all track ends according to tracking table\n",
    "track_ends = track_df.groupby('track_id')['frame_y'].max().reset_index()\n",
    "\n",
    "# exlude ends near end of the timelapse\n",
    "track_ends_before = track_ends[track_ends['frame_y'] <= img.shape[0] - frame_limit]\n",
    "print(len(track_ends_before))\n",
    "\n",
    "track_df_before = track_df[np.isin(track_df['track_id'].values, track_ends_before['track_id'].values)].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all track segments before the end (lag)\n",
    "track_df_before_lag = filter_time_window_below_per_group(track_df_before, 'track_id', 'frame', frame_window)\n",
    "print(len(track_df_before_lag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all track segments after the end, virtual extend at last known position (lead)\n",
    "\n",
    "# extend tracks at last known position\n",
    "track_df_before_ends = track_df_before.merge(track_ends_before)\n",
    "track_df_before_ends.rename(columns={'frame_y':'frame_start'}, inplace=True)\n",
    "\n",
    "track_df_before_ends['frame_end'] = track_df_before_ends['frame_start'] + frame_window + 1\n",
    "\n",
    "# Create a new column with the ranges\n",
    "track_df_before_ends['frame'] = track_df_before_ends.apply(lambda row: list(range(int(row['frame_start'])+1, int(row['frame_end']) if row['frame_end'] <= img.shape[0] else img.shape[0])), axis=1)\n",
    "\n",
    "# Explode the dataframe so each frame gets its own row\n",
    "df_expanded = track_df_before_ends.explode('frame').reset_index(drop=True)\n",
    "\n",
    "# Optional: drop frame_start and frame_end if no longer needed\n",
    "track_df_before_lead = df_expanded.drop(columns=['frame_start', 'frame_end'])\n",
    "print(len(track_df_before_lead))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate lag and lead track segment tables\n",
    "track_df_before_windows = pd.concat([track_df_before_lag, track_df_before_lead])\n",
    "print(len(track_df_before_windows))\n",
    "\n",
    "# make frame number relative to track end per track\n",
    "track_df_before_windows = track_df_before_windows.merge(track_ends.rename(columns={'frame_y': 'frame_mark'}), how = 'left', on = 'track_id')\n",
    "track_df_before_windows['frame_rel'] = track_df_before_windows['frame'] - track_df_before_windows['frame_mark']\n",
    "track_df_before_windows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # verify all track end windows on image level (circles on events to be isolated)\n",
    "# viewer.add_points(track_df_before_windows[['frame', 'centroid0', 'centroid1']],\n",
    "#                   name = 'track end markings',\n",
    "#                   face_color='transparent',\n",
    "#                   border_color='white',\n",
    "#                   border_width=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cropping table for all events from the full data\n",
    "\n",
    "# Crop buffer size in pixels (bbox size around spot in px)\n",
    "buffer = 50\n",
    "\n",
    "img_width = img.shape[-1]\n",
    "img_height = img.shape[-2]\n",
    "\n",
    "table_centering = track_df_before_windows.copy()\n",
    "table_centering.rename(columns={'centroid1': 'x', 'centroid0': 'y', 'frame': 't'}, inplace=True)\n",
    "\n",
    "# reindex track_id to index\n",
    "table_centering['index'] = table_centering.groupby('track_id').ngroup()\n",
    "\n",
    "# Build crop & padding table\n",
    "table_centering['x_int'] = table_centering['x'].round(0).astype(int)\n",
    "table_centering['y_int'] = table_centering['y'].round(0).astype(int)\n",
    "\n",
    "# calculate min and max X and Y by buffer from track centroid\n",
    "table_centering['x_min'] = table_centering['x_int'] - buffer\n",
    "table_centering['x_max'] = table_centering['x_int'] + buffer\n",
    "table_centering['y_min'] = table_centering['y_int'] - buffer\n",
    "table_centering['y_max'] = table_centering['y_int'] + buffer\n",
    "\n",
    "# calculate padding pixels in X and Y\n",
    "table_centering['pad_left'] = np.abs(np.clip(table_centering['x_min'], None, 0))\n",
    "table_centering['pad_right'] = np.abs(np.clip(table_centering['x_max'] - img_width, 0, None))\n",
    "table_centering['pad_top'] = np.abs(np.clip(table_centering['y_min'], None, 0))\n",
    "table_centering['pad_bottom'] = np.abs(np.clip(table_centering['y_max'] - img_height, 0, None))\n",
    "\n",
    "# clip min and max to image dims\n",
    "table_centering['x_min'] = np.clip(table_centering['x_min'], 0, None)\n",
    "table_centering['y_min'] = np.clip(table_centering['y_min'], 0, None)\n",
    "table_centering['x_max'] = np.clip(table_centering['x_max'], None, img_width)\n",
    "table_centering['y_max'] = np.clip(table_centering['y_max'], None, img_height)\n",
    "\n",
    "# sort and format crop table\n",
    "crop_table = table_centering.sort_values(by = ['t']).reset_index(drop = True)\n",
    "crop_table_np = crop_table[['index', 't', 'frame_rel', 'x_min', 'x_max', 'y_min', 'y_max', 'pad_left', 'pad_right', 'pad_top', 'pad_bottom']].to_numpy()\n",
    "# print(table_centering.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init empty np array for pasting the crops into\n",
    "sizes_stab = {\n",
    "    'T': frame_window*2 + 1,\n",
    "    'P': len(track_ends_before),\n",
    "    'X': buffer * 2,\n",
    "    'Y': buffer * 2}\n",
    "print(sizes_stab)\n",
    "\n",
    "stabilized_movie_img = np.zeros(shape = list(sizes_stab.values()), dtype = np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop imaging data and paste into np array according to crop table\n",
    "for i in tqdm(crop_table_np):\n",
    "    index, t, t_rel = i[:3]\n",
    "    t_rel = t_rel + frame_window\n",
    "    \n",
    "    # Crop parameters\n",
    "    x_min, x_max, y_min, y_max = i[-8:-4]\n",
    "    pad_left, pad_right, pad_top, pad_bottom = i[-4:]\n",
    "    \n",
    "    # Crop\n",
    "    img_crop = img[t, y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    # pad tile image if required\n",
    "    if (np.sum(i[-4:]) > 0):\n",
    "        pad_tuple = ((pad_top, pad_bottom), (pad_left, pad_right))\n",
    "        img_crop = np.pad(img_crop, pad_tuple, mode = 'constant')\n",
    "\n",
    "    # paste tile image\n",
    "    stabilized_movie_img[t_rel, index] = img_crop\n",
    "\n",
    "# Store array to disk\n",
    "np.save(path_movies + filename + \"_Stabilized.npy\", stabilized_movie_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize individual track results\n",
    "viewer = napari.Viewer()\n",
    "\n",
    "layerImage = viewer.add_image(stabilized_movie_img,\n",
    "                              blending = 'additive',\n",
    "                              contrast_limits=[0, np.max(intensity_peak)],#[np.quantile(stabilized_movie_img, q = .1), np.quantile(stabilized_movie_img, q = .995)],\n",
    "                              visible = True,\n",
    "                              gamma = 1)\n",
    "viewer.dims.axis_labels = ('Time', 'Track', 'Y', 'X')\n",
    "\n",
    "# reference circle\n",
    "viewer.add_points([buffer, buffer],\n",
    "                  size=5,\n",
    "                  opacity=.5,\n",
    "                  face_color='transparent',\n",
    "                  border_color='red',\n",
    "                  border_width=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indlude overlay: track_id & actual frame number\n",
    "minr = -5\n",
    "minc = 0\n",
    "maxr = 0\n",
    "maxc = buffer*2\n",
    "\n",
    "table_centering['overlay'] = 'track: ' + table_centering['track_id'].astype('str') + ' frame: ' + table_centering['t'].astype('str')\n",
    "\n",
    "text_parameters = {\n",
    "    'string': '{overlay}',\n",
    "    'size': 12,\n",
    "    'color': 'yellow',\n",
    "    'anchor': 'center',\n",
    "    'translation': [0, 0],\n",
    "    'opacity': 1\n",
    "}\n",
    "\n",
    "bbox_rect = np.array(\n",
    "    [[minr, minc], [maxr, minc], [maxr, maxc], [minr, maxc]]\n",
    ")\n",
    "\n",
    "overlay = []\n",
    "for i, row in table_centering.iterrows():\n",
    "    overlay.append(np.insert(bbox_rect, [0, 0], [row['frame_rel'] + frame_window, row['index']], axis = 1))\n",
    "\n",
    "overlay = np.stack(overlay)\n",
    "\n",
    "viewer.add_shapes(\n",
    "    overlay,\n",
    "    face_color = 'transparent',\n",
    "    edge_color = 'transparent',\n",
    "    edge_width = 1,\n",
    "    properties = table_centering,\n",
    "    # text = text_parameters,\n",
    "    text = 'overlay',\n",
    "    name = 'overlay track info',\n",
    "    opacity = 0.75\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation framework\n",
    "- points (maybe bind to annotated quadrants?)\n",
    "  - has this one been evaluated/seen?\n",
    "  - what category of event is this\n",
    "  - how many daughters are observed\n",
    "- join to index table & export\n",
    "- load data back to viewer (even if different filtering criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical True/False annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time agnostic point annotation layer (empty init)\n",
    "annot_bool = viewer.add_points(\n",
    "    data = np.zeros(shape = [0, 3]),\n",
    "    name = 'observed',\n",
    "    ndim = 3,\n",
    "    face_color = 'green',\n",
    "    border_color = 'transparent'\n",
    ")\n",
    "annot_bool.mode = 'add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch annotations from viewer (retains relative XY, is it needed??)\n",
    "annotation = pd.DataFrame(annot_bool.data.astype(int), columns= ['index', 'annot_y', 'annot_x'])\n",
    "annotation['observed'] = True\n",
    "print(annotation.tail())\n",
    "\n",
    "# deduplicate categorical annotations!\n",
    "annotation = annotation.groupby('index').last()\n",
    "\n",
    "# bind annotation to full table\n",
    "table = crop_table[crop_table['frame_rel'] == 0].merge(annotation, how = 'left', on = 'index')\n",
    "\n",
    "# show all annotated events in table\n",
    "table[table['observed'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count annotation (multipoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time agnostic point annotation layer (empty init)\n",
    "annot_count = viewer.add_points(\n",
    "    data = np.zeros(shape = [0, 3]),\n",
    "    name = 'count',\n",
    "    ndim = 3,\n",
    "    face_color = 'blue',\n",
    "    border_color = 'transparent'\n",
    ")\n",
    "annot_count.mode = 'add'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch annotations from viewer\n",
    "annotation = pd.DataFrame(annot_count.data.astype(int), columns= ['index', 'annot_y', 'annot_x'])\n",
    "print(annotation.tail())\n",
    "\n",
    "# count annotations per track! (XY per annot is destroyed)\n",
    "annotation = annotation.groupby('index').size().reset_index(name='count')\n",
    "\n",
    "# bind annotation to full table\n",
    "table = crop_table[crop_table['frame_rel'] == 0].merge(annotation, how = 'left', on = 'index')\n",
    "\n",
    "# show all annotated events in table\n",
    "table[table['count'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch mode tracking\n",
    "- all movies in the same folder\n",
    "- single position per file\n",
    "- no napari viewer output\n",
    "- no manual filtering (mask or toggling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list image files in directory\n",
    "img_files = [i for i in os.listdir(path_base) if i.endswith('.tif') or i.endswith('.nd2')]\n",
    "print(img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laptrack custom tracking metric\n",
    "def LAP_link_metric_intensity(c1, c2):\n",
    "    (frame1, label1), (frame2, label2) = c1, c2\n",
    "    if frame1 > frame2:\n",
    "        # switch spots in query if time order is wrong\n",
    "        tmp = (frame1, label1)\n",
    "        (frame1, label1) = (frame2, label2)\n",
    "        (frame2, label2) = tmp\n",
    "\n",
    "    # double frame index\n",
    "    ind = (frame1, frame2, label1, label2)\n",
    "    if ind in spots_int_rel.index:\n",
    "        if spots_int_rel.loc[ind][\"relIntDiff\"] >= max_relIntDiff:\n",
    "            # Too large difference intensity\n",
    "            value = np.float32(\"inf\")\n",
    "        else:\n",
    "            # Valid result\n",
    "            value = spots_int_rel.loc[ind][\"distanceScaled\"]\n",
    "    else:\n",
    "        # No candidates found\n",
    "        value = np.float32(\"inf\")\n",
    "    return value\n",
    "\n",
    "lap_tracker = LapTrack(\n",
    "    track_dist_metric = LAP_link_metric_intensity,\n",
    "    track_cost_cutoff = distance_threshold,\n",
    "    gap_closing_dist_metric =  LAP_link_metric_intensity,\n",
    "    gap_closing_max_frame_count = frame_gap_frames-2,\n",
    "    gap_closing_cost_cutoff = frame_gap_distance,\n",
    "    splitting_cost_cutoff = False,\n",
    "    merging_cost_cutoff = False,\n",
    "    parallel_backend = 'ray'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch mode all files in folder\n",
    "for filename in tqdm(img_files):\n",
    "    # Load image\n",
    "    path_img = path_base + filename\n",
    "    img = load_image(path_img, flatfield, img_ff[0], img_df)\n",
    "\n",
    "    # Spot detection\n",
    "    spots = SpotGeneric.detect_spots(img = img, thresholdLoG = detection_threshold)\n",
    "\n",
    "    # Spot feature extraction\n",
    "    spots, spots_int = SpotGeneric.measure_spots_intensities(spots = spots, radius = measurement_radius, img = img)\n",
    "    spots_int_rel, spots_in_range = SpotGeneric.metric_spots_relative_intensity(spots = spots, spotsIntensity = spots_int, gapMax = frame_gap_frames)\n",
    "    coordinate_df = pd.DataFrame(spots, columns = ['frame', 'centroid0', 'centroid1'])\n",
    "    coordinate_df['label'] = np.arange(len(spots))\n",
    "    coordinate_df['frame'] = coordinate_df['frame'].astype('int')\n",
    "\n",
    "    # Tracking\n",
    "    track_df, split_df, merge_df = lap_tracker.predict_dataframe(\n",
    "        coordinate_df, coordinate_cols=[\"frame\", \"label\"], only_coordinate_cols=False\n",
    "    )\n",
    "\n",
    "    # extract intensities and background subtraction\n",
    "    tracks = track_df.reset_index()\n",
    "    data_frames = np.zeros(shape = [len(tracks), 25, bbox_extent**2])\n",
    "\n",
    "    list_frames = np.unique(tracks['frame_y'].values)\n",
    "    for frame in list_frames:\n",
    "        spots_df = tracks[tracks['frame_y'] == frame]\n",
    "        spots_frame = spots_df[['centroid0', 'centroid1']].values.astype('int')\n",
    "        data_frames[spots_df.index] = SpotGeneric.extract_bbox_grid_intensity(img[frame], spots_frame, bbox_extent)\n",
    "\n",
    "    bbox_indices = SpotGeneric.index_bbox_grid_tiers()\n",
    "    intensity_peak, intensity_bg, intensity_peaksub, coef = SpotGeneric.parse_bbox_grid_intensity(data_frames, bbox_indices, thr_bg = coef_var_thr)\n",
    "\n",
    "    # join intenstity data to tracking table\n",
    "    tracks_join = tracks.join(\n",
    "        pd.DataFrame({\n",
    "            'intensity': intensity_peak,\n",
    "            'background': intensity_bg,\n",
    "            'intensity_bg': intensity_peaksub\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # write to file\n",
    "    df_export = tracks_join[['track_id', 'centroid1', 'centroid0', 'frame_y']].copy()\n",
    "\n",
    "    df_export.rename(columns={\n",
    "        'centroid1': 'x',\n",
    "        'centroid0': 'y',\n",
    "        'frame_y': 't'\n",
    "    }, inplace = True)\n",
    "\n",
    "    if track_index_rearange_length:\n",
    "        # re-index track_id value by tracklength\n",
    "        key_counts = df_export['track_id'].value_counts()\n",
    "\n",
    "        # Step 2: Create a rank based on the counts (largest group gets index 1)\n",
    "        rank_mapping = {key: rank + 1 for rank, key in enumerate(key_counts.index)}\n",
    "\n",
    "        # Step 3: Map the new rank to the key column\n",
    "        df_export['track_id'] = df_export['track_id'].map(rank_mapping)\n",
    "\n",
    "    df_export.to_csv(path_base + filename + \"_tracks_all.csv\")\n",
    "\n",
    "    # Track length filter (n obs)\n",
    "    tracks_length_cutoff = TrackFilter.track_filter_min_length(tracks_join, track_length_minimum)\n",
    "    # Local spot density per track\n",
    "    tracks_density_cutoff = TrackFilter.track_filter_density(tracks_join, spots, track_density_maximum, supplement=True)\n",
    "    # Tracks initiate window filter (before or after frame number)\n",
    "    tracks_start_window = TrackFilter.track_filter_start_window(tracks_join, retain_before_frame, frame_threshold)\n",
    "    # Concatenate and parse track_id allow lists\n",
    "    tracks_retain = reduce(\n",
    "        np.intersect1d, (\n",
    "            tracks_inclusion_cutoff,\n",
    "            tracks_exclusion_cutoff,\n",
    "            tracks_length_cutoff,\n",
    "            tracks_density_cutoff,\n",
    "            tracks_start_window\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print('n tracks to retain: ' + str(len(tracks_retain)))\n",
    "\n",
    "    # actual tracking table filter\n",
    "    track_df_filter = tracks_join[np.isin(tracks_join['track_id'].values, tracks_retain)]\n",
    "\n",
    "    # write to file\n",
    "    df_export = track_df_filter[['track_id', 'centroid1', 'centroid0', 'frame_y']].copy()\n",
    "\n",
    "    df_export.rename(columns={\n",
    "        'centroid1': 'x',\n",
    "        'centroid0': 'y',\n",
    "        'frame_y': 't'\n",
    "    }, inplace = True)\n",
    "\n",
    "    if track_index_rearange_length:\n",
    "        # re-index track_id value by tracklength\n",
    "        key_counts = df_export['track_id'].value_counts()\n",
    "\n",
    "        # Step 2: Create a rank based on the counts (largest group gets index 1)\n",
    "        rank_mapping = {key: rank + 1 for rank, key in enumerate(key_counts.index)}\n",
    "\n",
    "        # Step 3: Map the new rank to the key column\n",
    "        df_export['track_id'] = df_export['track_id'].map(rank_mapping)\n",
    "\n",
    "    df_export.to_csv(path_base + filename + \"_tracks_filter.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "__apptainer__",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
